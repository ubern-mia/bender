{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying the dermamnist data set (initial version: 01)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ubern-mia/bender/episode03/dermamnist_v1_initial.ipynb)\n",
    "\n",
    "This is a naive version of training a simple 4 layer CNN.\n",
    "\n",
    "First, we (optionally install and) import the libraries we depend on, and choose the type of compute resource that is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install medmnist torch torchvision tqdm matplotlib sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the torch.device you will use.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a helper function to load the data from the medmnist data set (we use dermamnist, but it should be easy to modify this to any of the other flavors available). To do this, change the 'data_flag' variable to be \"pathmnist\", for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(flag):\n",
    "    \"\"\"\n",
    "    load_datasets loads the dermamnist data.\n",
    "    'flag' takes two options:\n",
    "        'train': loads the training set as first output, validation set as second.\n",
    "        'test' : loads the training set as first output, test set as second.\n",
    "    \"\"\"\n",
    "\n",
    "    data_flag = \"dermamnist\"\n",
    "    download = True\n",
    "    info = INFO[data_flag]\n",
    "\n",
    "    DataClass = getattr(medmnist, info[\"python_class\"])\n",
    "\n",
    "    transform_medmnist = transforms.Compose([transforms.ToTensor(), transforms.Pad(2)])\n",
    "\n",
    "    data_train = DataClass(\n",
    "        split=\"train\", transform=transform_medmnist, download=download\n",
    "    )\n",
    "    if flag == \"train\":\n",
    "        data_next = DataClass(\n",
    "            split=\"val\", transform=transform_medmnist, download=download\n",
    "        )\n",
    "    elif flag == \"test\":\n",
    "        data_next = DataClass(\n",
    "            split=\"test\", transform=transform_medmnist, download=download\n",
    "        )\n",
    "\n",
    "    return data_train, data_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a simple class called CNN, which holds our model architecture. In this version, it is a simple four layer Convolutional network, with four blocks of Conv + ReLU + Batch Norm layers, all with 64 filters, and all but the first one with size 3-by-3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple 4 layered CNN to run classification on dermamnist.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Definition of layers in the CNN.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, (5, 5), padding=2, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            # (32, 32, 32)\n",
    "            nn.Conv2d(64, 64, (3, 3), padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            # (32, 32, 32)\n",
    "            nn.Conv2d(64, 64, (3, 3), padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            # (32, 32, 32)\n",
    "            nn.Conv2d(64, 64, (3, 3), padding=1, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            # (64, 16, 16)\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.classifier = nn.Sequential(nn.Linear(64, 7))\n",
    "\n",
    "    def forward(self, in_tensor):\n",
    "        \"\"\"\n",
    "        Forward pass through the CNN.\n",
    "        \"\"\"\n",
    "\n",
    "        in_tensor = self.features(in_tensor)\n",
    "        in_tensor = self.avgpool(in_tensor)\n",
    "        in_tensor = torch.reshape(in_tensor, (-1, 64))\n",
    "        return self.classifier(in_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these components of our model setup, we then write up the training and test loops. The training loop here includes setting up the model hyperparameters and the loss function. This script also includes code to log training loss and validation accuracies at every 50 iterations, and, the end of each epoch respectively. These plots are stored as .png files to analyze after, for performance and potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(output_path: str = None, batch_size: int = 8, num_epochs: int = 100):\n",
    "    \"\"\"\n",
    "    Model training loop, including setting up hyperparameters.\n",
    "    \"\"\"\n",
    "\n",
    "    if output_path is None:\n",
    "        print(\"output_path needs to be setup. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    data_train, data_val = load_datasets(\"train\")\n",
    "    # Define the PyTorch data loaders for the training and test datasets.\n",
    "    # Use the given batch_size and remember that the training loader should\n",
    "    # shuffle the batches each epoch.\n",
    "    loader_train = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "    loader_val = DataLoader(data_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Define the model architecture.\n",
    "    model = CNN()\n",
    "\n",
    "    # Compute the number of parameters of the model\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Number of parameters: {num_params}\")\n",
    "\n",
    "    # Setup the model training hyperparameters.\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.000005, momentum=0.5)\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Iteration counter\n",
    "    it = 0\n",
    "    train_loss = []\n",
    "    val_acc = []\n",
    "\n",
    "    # Keep track of the best performance reached so far.\n",
    "    best_accuracy = 0\n",
    "\n",
    "    # Number of iterations required in one epoch\n",
    "    epoch_length = len(loader_train)\n",
    "\n",
    "    # Repeat training the given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        print(f\"Starting epoch {epoch + 1}/{num_epochs}...\")\n",
    "\n",
    "        # Run one epoch\n",
    "        for batch in tqdm(loader_train):\n",
    "\n",
    "            it += 1\n",
    "\n",
    "            # REMEMBER TO SET THE TRAINING STATE OF THE MODEL.\n",
    "            # Call .train() before training and .eval() before evaluation every\n",
    "            # time!!\n",
    "            model.train()\n",
    "            inputs = batch[0]\n",
    "            labels = batch[1]\n",
    "            labels = labels.squeeze().long()\n",
    "\n",
    "            # Zero your gradients for every batch!\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Make predictions for this batch\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Adjust learning weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log the training loss once every 50 iterations\n",
    "            if (it % 50) == 0:\n",
    "                train_loss.append([loss, it])\n",
    "\n",
    "            # Run validation and save the model once every epoch.\n",
    "            # You could put this code outside the inner training loop, but\n",
    "            # doing it here allows you to run validation more than once per epoch.\n",
    "            if (it % epoch_length) == 0:\n",
    "\n",
    "                batch = next(iter(loader_val))\n",
    "\n",
    "                inputs = batch[0]\n",
    "                labels = batch[1]\n",
    "                labels = labels.squeeze().long()\n",
    "\n",
    "                # Zero your gradients for every batch!\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Make predictions for this batch\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Loop over the metrics for validation, loss and accuracy.\n",
    "                metrics = evaluate_model(model, loader_val)\n",
    "\n",
    "                # Loop over the metrics and log them\n",
    "                for key in metrics.keys():\n",
    "                    val_acc.append([metrics[key], it])\n",
    "\n",
    "                accuracy = metrics[\"accuracy\"]\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    model_file = os.path.join(output_path, \"best_model.pt\")\n",
    "                    # Save the model to a `model_file`.\n",
    "                    torch.save(model.state_dict(), model_file)\n",
    "\n",
    "                print(f\"Current accuracy is {accuracy}, and best is: {best_accuracy}.\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(\n",
    "        [it for loss, it in train_loss],\n",
    "        [loss.detach().item() for loss, it in train_loss],\n",
    "    )\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Training loss\")\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(output_path, \"train_loss.png\"))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot([it for acc, it in val_acc], [acc for acc, it in val_acc])\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Validation accuracy\")\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(output_path, \"val_acc.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test function is relatively simpler: this requires a model that is trained already, and uses the same 'load_datasets' function written earlier to now load the test set and evaluate the generalization capacity of our model. The evaluation is done using another helper function called 'evaluate_model', which is reused in the training loop as well! :-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_path: str=None):\n",
    "    \"\"\"\n",
    "    Test model after training.\n",
    "    \"\"\"\n",
    "\n",
    "    model = CNN()\n",
    "    if model_path is None:\n",
    "        print(\"model_path needs to be specified, which includes 'best_model.pt'.\")\n",
    "        return\n",
    "\n",
    "    model.load_state_dict(torch.load(os.path.join(model_path, \"best_model.pt\")))\n",
    "\n",
    "    # Print model's state_dict\n",
    "    print(\"Model's state_dict:\")\n",
    "    for param_tensor in model.state_dict():\n",
    "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Number of parameters: {num_params}\")\n",
    "\n",
    "    _, data_test = load_datasets(\"test\")\n",
    "    loader_test = DataLoader(data_test, batch_size=8, shuffle=False)\n",
    "\n",
    "    metrics = evaluate_model(model, loader_test)\n",
    "    print(f\"Test accuracy is: {metrics['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function evaluates our model (both during validation and testing): and prints out the classification report, as well as the accuracy of the model. Note that it is very important to set the model to 'eval()' mode, and use the torch.no_grad() decorator, without which bad things can happen ;-). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model(model: nn.Module, loader: DataLoader):\n",
    "    \"\"\"\n",
    "    Evaluate model while training.\n",
    "    \"\"\"\n",
    "\n",
    "    data_flag = \"dermamnist\"\n",
    "    info = INFO[data_flag]\n",
    "\n",
    "    # Evaluate the model with the given data loader.\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    metrics = {}\n",
    "\n",
    "    label_list = []\n",
    "    pred_list = []\n",
    "\n",
    "    for data in loader:\n",
    "        images, labels = data[0], data[1]\n",
    "        labels = labels.squeeze().long()\n",
    "\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = model(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for x in predicted.numpy().tolist():\n",
    "            pred_list.append(x)\n",
    "        for x in labels.numpy().tolist():\n",
    "            label_list.append(x)\n",
    "\n",
    "    print(\n",
    "        classification_report(\n",
    "            label_list, pred_list, target_names=list(info[\"label\"].values()), digits=4\n",
    "        )\n",
    "    )\n",
    "\n",
    "    metrics[\"accuracy\"] = correct / total\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, choose a mode to run these functions: in TRAINING_MODE, the model is trained on the training set we load from medmnist, and otherwise, it is tested using the testing set, for evaluating the generalization capability. Change the output_path to be any writable folder on your computer to save the plots for the training loss and validation accuracy. The metrics while training are otherwise printed on the terminal/console. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_mode = True\n",
    "output_path = \"./dermamnist_v1\"\n",
    "\n",
    "if training_mode:\n",
    "    train(output_path)\n",
    "else:\n",
    "    test(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82b48cacd71dc9ae8e4d05989d0458e218774c4e2616f82d4a1b1aa074fea350"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
